# S3 Storage Buckets

- bucket == directory
- object == file
- data partitioning
  - Kinesis Data Firehose for dynamic partioning
- data lake
- different classes and prices and performance
- management tab > lifecycle rules -> 
  - automate changing object classes
  - auto delete
  - 5 different rule actions:
    - move current versionsof objects between storage classes,
    - move noncurrent versions of object between storage classes,
    - expire current versions of objects,
    - permanently delete noncurrent versions of objects,
    - delete expired objects delete markers or incomplete multi-part uploads.
- S3 versioning
  - eg "delete" marker
- S3 analytics
- for classes keep in mind the min storage time charged

## storage classses

- Amazon S3 Standard - General Purpose

S3 Standard has 99.99 availability.

It's going to be used for frequently excess data.

This is the kind of storage you use by default,

and has low latency and high throughput.

It can sustain two concurrent facility failures

on the side of AWS, and the use cases

for it is going to be big data analytics,

mobile and gaming application,

as well as content distribution.

- Amazon S3 Standard-Infrequent Access (IA)

less frequently accessed

but requires rapid access when needed.

It's going to be lower cost than S3 - Standard,

but you will have a cost on retrieval.

So the S3 - Standard IA is 99.9% availability,

so a bit less available, and the use case

for it is going to be disaster recovery and backups.

- Amazon S3 One Zone-Infrequent Access

store secondary copy of backups

of maybe on premises data, or data you can recreate.

- Glacier

So it's low-cost object storage meant

for archiving and backup.

- Amazon S3 Glacier Instant Retrieval

backup, but you need to access it

within milliseconds.

- Amazon S3 Glacier Flexible Rerieval

Expedited where you get the data back

between one and five minutes.

You have Standard to get the data back

between three to five hours,

or Bulk, which is free,

- Amazon S3 Glacier Deep Archive

We have a Standard of 12 hours, and Bulk of 48 hours.

So you may be ready to wait a lot

of time to retrieve your data,

but it's going to give you the lowest cost.

- Amazon S3 Intelligent Tiering

allow you to move objects

between Access Tiers based on usage patterns.

And for this you're going to incur a small

monthly monitoring fee, and auto-tiering fee.

Frequent Access tier that's automatic.

This the default tier.

Then we have the Infrequent Access tier for objects

not accessed, for example, for 30 days.

Then you have the Archive Instant Access tier

automatic as well for objects not accessed over 90 days.

And then the Archive Access tier, that's optional,

and you can configure from 90 days to 700 plus days.

And then you have the Deep Archive Access tier,

also optional, that you can configure

for objects that haven't been accessed

between 180 days to 700 plus days.

## Define

- Durability
- Availability

## Amazon S3 Security

- user based - IAM policy
- resource based 
  - bucket policies (can make public) <- most common
  - object access control list ACL - fine grain (can be disabled)
  - bucket access control list ACL - less common (can be disabled)
- IAM principal can access S3 object if user IAM permissions ALLOW it OR resource policy allows it AND there's no explicit DENY
- encrypt objects

## S3 bucket policies

- json based policies
  - resource
  - effect
    - "Allow" or "Deny"
  - action
  - principal
  - Q: what's the difference between "Effect": "Allow" and "Action": ["s3:GetObject"] ?
- attach policy to bucket
  - if going through IAM users, assign IAM permissions to user through a policy
  - or IAM role with correct IAM permissions for eg EC2 to access S3 bucket
- Cross Account access - use bucket policy
- Bucket settings for Block Public Access to prevent data leaks which overrides public bucket policy
  - can be set at the account level so nothing is ever public
  - prevents dumb users who would set bad bucket policies

## S3 Object Encryption

- Server Side Encryption (SSE):
  - sse with Amazon S3-Managed Keys (SSE-S3) <- default
    - keys handled managed and owned by AWS
    - encrypted server-side
    - AES-256
    - Must set header "x-amz-server-side-encryption": "AES256" to request S3 to encrypt
    - User uploads file with correct header, S3 encrypts with key and saves encrypted to bucket
  - sse with Key Management Service (AWS KMS)
    - You want to manage keys yourself with KMS
    - can audit key usage using CloudTrail
    - header "x-amz-server-side-encryption": "aws:kms", and specify key want to use, key to encrypt comes from KMS
    - limitations:
      - must rely on KMS API
      - GenerateDataKey and Decrypt requests to KMS
      - can bottleneck, check quota per second (based on region 5k to 30k req/sec)
        - though can increase in Service Quotas Console
      - if high throughput use different SSE
  - sse with Customer-Provided Keys (SSE-C)
    - keys managed by customer outside of AWS
    - must pass key in header
    - must use HTTPS
  - S3 can be HTTP or HTTPS
    - HTTPS recommended
  - can force HTTPS (Encryption in Transit) with a bucket policy, deny bool if "aws:SecureTransport": "false" "Principal":"*"
- Client side encryption
  - can use Client-Side Encryption Library
  - clients send encrypted data themselves
- ðŸ’¡ understand which ones for which situations for the exam
- can set encryption at bucket creation
- if versioning enabled, then changing object's sse with create a new version of the object with updated settings and a new last modified date
- if use KMS, there is a default KMS key for free (can create a KMS key to for money) called the S3 Bucket Key, used by SSE-S3 by default
- Bucket policies are evaluated before "Default Encryption"
